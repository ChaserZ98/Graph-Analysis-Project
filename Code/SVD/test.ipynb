{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_SVD import my_SVD\n",
    "from funk_svd import SVD\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"../../Data/Review Sample/\"\n",
    "\n",
    "train = pd.read_csv(dir + \"train_100k.csv\", on_bad_lines=\"skip\")\n",
    "valid = pd.read_csv(dir + \"validation_100k.csv\", on_bad_lines=\"skip\")\n",
    "test = pd.read_csv(dir + \"test_100k.csv\", on_bad_lines=\"skip\")\n",
    "\n",
    "input_train = train[['reviewerID', 'asin', 'overall']].rename(columns={\"reviewerID\": \"u_id\", \"asin\": \"i_id\", \"overall\": \"rating\"})\n",
    "input_valid = valid[['reviewerID', 'asin', 'overall']].rename(columns={\"reviewerID\": \"u_id\", \"asin\": \"i_id\", \"overall\": \"rating\"})\n",
    "input_test = test[['reviewerID', 'asin', 'overall']].rename(columns={\"reviewerID\": \"u_id\", \"asin\": \"i_id\", \"overall\": \"rating\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_train = pd.read_csv(\"../../Data/NLP/train.csv\")[['reviewerID', 'asin', 'textRate']].rename(columns={\"reviewerID\": \"u_id\", \"asin\": \"i_id\", \"textRate\": \"nlp rating\"})\n",
    "nlp_valid = pd.read_csv(\"../../Data/NLP/validation.csv\")[['reviewerID', 'asin', 'textRate']].rename(columns={\"reviewerID\": \"u_id\", \"asin\": \"i_id\", \"textRate\": \"nlp rating\"})\n",
    "nlp_test = pd.read_csv(\"../../Data/NLP/test.csv\")[['reviewerID', 'asin', 'textRate']].rename(columns={\"reviewerID\": \"u_id\", \"asin\": \"i_id\", \"textRate\": \"nlp rating\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train['nlp rating'] = nlp_train['nlp rating']\n",
    "input_valid['nlp rating'] = nlp_test['nlp rating']\n",
    "input_test['nlp rating'] = nlp_test['nlp rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "reg = 0.005\n",
    "n_epochs = 100\n",
    "n_factors = 30\n",
    "early_stopping = True\n",
    "shuffle = False\n",
    "min_rating = 1\n",
    "max_rating = 5\n",
    "min_delta = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rating only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_svd = my_SVD(lr=lr, reg=reg, n_epochs=n_epochs, n_factors=n_factors, early_stopping=early_stopping, shuffle=shuffle, min_rating=min_rating, max_rating=max_rating, min_delta=min_delta, mode=\"rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data...\n",
      "\n",
      "Preprocessing data...\n",
      "\n",
      "Epoch 1/100  | train_loss: 1.216 - train_rmse: 1.103 - train_mae: 0.859 - val_loss: 1.218 - val_rmse: 1.104 - val_mae: 0.861 - took 1.3 sec\n",
      "Epoch 2/100  | train_loss: 1.177 - train_rmse: 1.085 - train_mae: 0.845 - val_loss: 1.198 - val_rmse: 1.094 - val_mae: 0.852 - took 0.5 sec\n",
      "Epoch 3/100  | train_loss: 1.144 - train_rmse: 1.070 - train_mae: 0.832 - val_loss: 1.182 - val_rmse: 1.087 - val_mae: 0.844 - took 0.5 sec\n",
      "Epoch 4/100  | train_loss: 1.115 - train_rmse: 1.056 - train_mae: 0.822 - val_loss: 1.169 - val_rmse: 1.081 - val_mae: 0.838 - took 0.5 sec\n",
      "Epoch 5/100  | train_loss: 1.089 - train_rmse: 1.044 - train_mae: 0.812 - val_loss: 1.158 - val_rmse: 1.076 - val_mae: 0.833 - took 0.5 sec\n",
      "Epoch 6/100  | train_loss: 1.066 - train_rmse: 1.032 - train_mae: 0.802 - val_loss: 1.149 - val_rmse: 1.072 - val_mae: 0.828 - took 0.5 sec\n",
      "Epoch 7/100  | train_loss: 1.044 - train_rmse: 1.022 - train_mae: 0.794 - val_loss: 1.141 - val_rmse: 1.068 - val_mae: 0.823 - took 0.5 sec\n",
      "Epoch 8/100  | train_loss: 1.024 - train_rmse: 1.012 - train_mae: 0.786 - val_loss: 1.135 - val_rmse: 1.065 - val_mae: 0.819 - took 0.5 sec\n",
      "Epoch 9/100  | train_loss: 1.006 - train_rmse: 1.003 - train_mae: 0.778 - val_loss: 1.129 - val_rmse: 1.062 - val_mae: 0.816 - took 0.5 sec\n",
      "Epoch 10/100 | train_loss: 0.988 - train_rmse: 0.994 - train_mae: 0.771 - val_loss: 1.124 - val_rmse: 1.060 - val_mae: 0.813 - took 0.5 sec\n",
      "Epoch 11/100 | train_loss: 0.972 - train_rmse: 0.986 - train_mae: 0.764 - val_loss: 1.119 - val_rmse: 1.058 - val_mae: 0.809 - took 0.5 sec\n",
      "Epoch 12/100 | train_loss: 0.957 - train_rmse: 0.978 - train_mae: 0.757 - val_loss: 1.115 - val_rmse: 1.056 - val_mae: 0.807 - took 0.5 sec\n",
      "Epoch 13/100 | train_loss: 0.943 - train_rmse: 0.971 - train_mae: 0.751 - val_loss: 1.112 - val_rmse: 1.054 - val_mae: 0.804 - took 0.5 sec\n",
      "Epoch 14/100 | train_loss: 0.929 - train_rmse: 0.964 - train_mae: 0.745 - val_loss: 1.108 - val_rmse: 1.053 - val_mae: 0.802 - took 0.5 sec\n",
      "Epoch 15/100 | train_loss: 0.916 - train_rmse: 0.957 - train_mae: 0.739 - val_loss: 1.106 - val_rmse: 1.052 - val_mae: 0.799 - took 0.5 sec\n",
      "Epoch 16/100 | train_loss: 0.904 - train_rmse: 0.951 - train_mae: 0.734 - val_loss: 1.103 - val_rmse: 1.050 - val_mae: 0.797 - took 0.5 sec\n",
      "Epoch 17/100 | train_loss: 0.892 - train_rmse: 0.945 - train_mae: 0.729 - val_loss: 1.101 - val_rmse: 1.049 - val_mae: 0.795 - took 0.5 sec\n",
      "Epoch 18/100 | train_loss: 0.881 - train_rmse: 0.939 - train_mae: 0.723 - val_loss: 1.099 - val_rmse: 1.048 - val_mae: 0.793 - took 0.5 sec\n",
      "Epoch 19/100 | train_loss: 0.871 - train_rmse: 0.933 - train_mae: 0.719 - val_loss: 1.098 - val_rmse: 1.048 - val_mae: 0.791 - took 0.5 sec\n",
      "Epoch 20/100 | train_loss: 0.860 - train_rmse: 0.928 - train_mae: 0.714 - val_loss: 1.096 - val_rmse: 1.047 - val_mae: 0.790 - took 0.5 sec\n",
      "Epoch 21/100 | train_loss: 0.851 - train_rmse: 0.922 - train_mae: 0.709 - val_loss: 1.095 - val_rmse: 1.046 - val_mae: 0.788 - took 0.5 sec\n",
      "Epoch 22/100 | train_loss: 0.841 - train_rmse: 0.917 - train_mae: 0.705 - val_loss: 1.094 - val_rmse: 1.046 - val_mae: 0.786 - took 0.5 sec\n",
      "Epoch 23/100 | train_loss: 0.832 - train_rmse: 0.912 - train_mae: 0.700 - val_loss: 1.093 - val_rmse: 1.045 - val_mae: 0.785 - took 0.5 sec\n",
      "Epoch 24/100 | train_loss: 0.823 - train_rmse: 0.907 - train_mae: 0.696 - val_loss: 1.092 - val_rmse: 1.045 - val_mae: 0.784 - took 0.5 sec\n",
      "Epoch 25/100 | train_loss: 0.815 - train_rmse: 0.903 - train_mae: 0.692 - val_loss: 1.091 - val_rmse: 1.044 - val_mae: 0.782 - took 0.5 sec\n",
      "Epoch 26/100 | train_loss: 0.807 - train_rmse: 0.898 - train_mae: 0.688 - val_loss: 1.090 - val_rmse: 1.044 - val_mae: 0.781 - took 0.5 sec\n",
      "Epoch 27/100 | train_loss: 0.799 - train_rmse: 0.894 - train_mae: 0.684 - val_loss: 1.090 - val_rmse: 1.044 - val_mae: 0.780 - took 0.5 sec\n",
      "Epoch 28/100 | train_loss: 0.791 - train_rmse: 0.890 - train_mae: 0.680 - val_loss: 1.089 - val_rmse: 1.044 - val_mae: 0.779 - took 0.5 sec\n",
      "Epoch 29/100 | train_loss: 0.784 - train_rmse: 0.885 - train_mae: 0.677 - val_loss: 1.089 - val_rmse: 1.043 - val_mae: 0.777 - took 0.5 sec\n",
      "Epoch 30/100 | train_loss: 0.777 - train_rmse: 0.881 - train_mae: 0.673 - val_loss: 1.088 - val_rmse: 1.043 - val_mae: 0.776 - took 0.5 sec\n",
      "Epoch 31/100 | train_loss: 0.770 - train_rmse: 0.877 - train_mae: 0.669 - val_loss: 1.088 - val_rmse: 1.043 - val_mae: 0.775 - took 0.5 sec\n",
      "Epoch 32/100 | train_loss: 0.763 - train_rmse: 0.874 - train_mae: 0.666 - val_loss: 1.088 - val_rmse: 1.043 - val_mae: 0.774 - took 0.5 sec\n",
      "\n",
      "Training took 21 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<my_SVD.my_SVD at 0x1f92758b2b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_svd.fit(input_train, input_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train Loss</th>\n",
       "      <th>Train RMSE</th>\n",
       "      <th>Train MAE</th>\n",
       "      <th>Valid Loss</th>\n",
       "      <th>Valid RMSE</th>\n",
       "      <th>Valid MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.215923</td>\n",
       "      <td>1.102689</td>\n",
       "      <td>0.858688</td>\n",
       "      <td>1.217946</td>\n",
       "      <td>1.103606</td>\n",
       "      <td>0.860731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.177140</td>\n",
       "      <td>1.084961</td>\n",
       "      <td>0.844670</td>\n",
       "      <td>1.197614</td>\n",
       "      <td>1.094356</td>\n",
       "      <td>0.851815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.144223</td>\n",
       "      <td>1.069684</td>\n",
       "      <td>0.832487</td>\n",
       "      <td>1.181837</td>\n",
       "      <td>1.087123</td>\n",
       "      <td>0.844481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.115261</td>\n",
       "      <td>1.056059</td>\n",
       "      <td>0.821550</td>\n",
       "      <td>1.168965</td>\n",
       "      <td>1.081187</td>\n",
       "      <td>0.838211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.089298</td>\n",
       "      <td>1.043694</td>\n",
       "      <td>0.811556</td>\n",
       "      <td>1.158213</td>\n",
       "      <td>1.076203</td>\n",
       "      <td>0.832724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.065733</td>\n",
       "      <td>1.032343</td>\n",
       "      <td>0.802313</td>\n",
       "      <td>1.149096</td>\n",
       "      <td>1.071959</td>\n",
       "      <td>0.827845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.044144</td>\n",
       "      <td>1.021834</td>\n",
       "      <td>0.793693</td>\n",
       "      <td>1.141282</td>\n",
       "      <td>1.068308</td>\n",
       "      <td>0.823457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.024222</td>\n",
       "      <td>1.012039</td>\n",
       "      <td>0.785604</td>\n",
       "      <td>1.134531</td>\n",
       "      <td>1.065144</td>\n",
       "      <td>0.819477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.005728</td>\n",
       "      <td>1.002860</td>\n",
       "      <td>0.777978</td>\n",
       "      <td>1.128659</td>\n",
       "      <td>1.062384</td>\n",
       "      <td>0.815852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.988474</td>\n",
       "      <td>0.994220</td>\n",
       "      <td>0.770757</td>\n",
       "      <td>1.123526</td>\n",
       "      <td>1.059965</td>\n",
       "      <td>0.812530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.972306</td>\n",
       "      <td>0.986056</td>\n",
       "      <td>0.763896</td>\n",
       "      <td>1.119020</td>\n",
       "      <td>1.057837</td>\n",
       "      <td>0.809468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.957102</td>\n",
       "      <td>0.978316</td>\n",
       "      <td>0.757357</td>\n",
       "      <td>1.115052</td>\n",
       "      <td>1.055960</td>\n",
       "      <td>0.806633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.942756</td>\n",
       "      <td>0.970956</td>\n",
       "      <td>0.751111</td>\n",
       "      <td>1.111549</td>\n",
       "      <td>1.054300</td>\n",
       "      <td>0.803996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.929181</td>\n",
       "      <td>0.963940</td>\n",
       "      <td>0.745132</td>\n",
       "      <td>1.108451</td>\n",
       "      <td>1.052830</td>\n",
       "      <td>0.801533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.916302</td>\n",
       "      <td>0.957237</td>\n",
       "      <td>0.739396</td>\n",
       "      <td>1.105707</td>\n",
       "      <td>1.051526</td>\n",
       "      <td>0.799225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.904055</td>\n",
       "      <td>0.950818</td>\n",
       "      <td>0.733881</td>\n",
       "      <td>1.103275</td>\n",
       "      <td>1.050369</td>\n",
       "      <td>0.797059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.892383</td>\n",
       "      <td>0.944660</td>\n",
       "      <td>0.728572</td>\n",
       "      <td>1.101118</td>\n",
       "      <td>1.049342</td>\n",
       "      <td>0.795023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.881238</td>\n",
       "      <td>0.938743</td>\n",
       "      <td>0.723454</td>\n",
       "      <td>1.099208</td>\n",
       "      <td>1.048431</td>\n",
       "      <td>0.793105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.870576</td>\n",
       "      <td>0.933047</td>\n",
       "      <td>0.718511</td>\n",
       "      <td>1.097516</td>\n",
       "      <td>1.047624</td>\n",
       "      <td>0.791294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.860361</td>\n",
       "      <td>0.927557</td>\n",
       "      <td>0.713731</td>\n",
       "      <td>1.096020</td>\n",
       "      <td>1.046910</td>\n",
       "      <td>0.789579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.850558</td>\n",
       "      <td>0.922257</td>\n",
       "      <td>0.709104</td>\n",
       "      <td>1.094699</td>\n",
       "      <td>1.046279</td>\n",
       "      <td>0.787955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.841137</td>\n",
       "      <td>0.917135</td>\n",
       "      <td>0.704621</td>\n",
       "      <td>1.093538</td>\n",
       "      <td>1.045724</td>\n",
       "      <td>0.786414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.832071</td>\n",
       "      <td>0.912179</td>\n",
       "      <td>0.700273</td>\n",
       "      <td>1.092519</td>\n",
       "      <td>1.045237</td>\n",
       "      <td>0.784949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.823335</td>\n",
       "      <td>0.907378</td>\n",
       "      <td>0.696053</td>\n",
       "      <td>1.091631</td>\n",
       "      <td>1.044811</td>\n",
       "      <td>0.783554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.814908</td>\n",
       "      <td>0.902723</td>\n",
       "      <td>0.691951</td>\n",
       "      <td>1.090860</td>\n",
       "      <td>1.044442</td>\n",
       "      <td>0.782226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.806770</td>\n",
       "      <td>0.898204</td>\n",
       "      <td>0.687961</td>\n",
       "      <td>1.090196</td>\n",
       "      <td>1.044124</td>\n",
       "      <td>0.780959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.798903</td>\n",
       "      <td>0.893814</td>\n",
       "      <td>0.684075</td>\n",
       "      <td>1.089629</td>\n",
       "      <td>1.043853</td>\n",
       "      <td>0.779747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.791289</td>\n",
       "      <td>0.889544</td>\n",
       "      <td>0.680290</td>\n",
       "      <td>1.089152</td>\n",
       "      <td>1.043625</td>\n",
       "      <td>0.778588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.783915</td>\n",
       "      <td>0.885390</td>\n",
       "      <td>0.676599</td>\n",
       "      <td>1.088757</td>\n",
       "      <td>1.043435</td>\n",
       "      <td>0.777480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.776766</td>\n",
       "      <td>0.881343</td>\n",
       "      <td>0.672999</td>\n",
       "      <td>1.088436</td>\n",
       "      <td>1.043281</td>\n",
       "      <td>0.776418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.769828</td>\n",
       "      <td>0.877399</td>\n",
       "      <td>0.669483</td>\n",
       "      <td>1.088183</td>\n",
       "      <td>1.043160</td>\n",
       "      <td>0.775401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.763092</td>\n",
       "      <td>0.873551</td>\n",
       "      <td>0.666049</td>\n",
       "      <td>1.087994</td>\n",
       "      <td>1.043070</td>\n",
       "      <td>0.774426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Train Loss  Train RMSE  Train MAE  Valid Loss  Valid RMSE  Valid MAE\n",
       "0     1.215923    1.102689   0.858688    1.217946    1.103606   0.860731\n",
       "1     1.177140    1.084961   0.844670    1.197614    1.094356   0.851815\n",
       "2     1.144223    1.069684   0.832487    1.181837    1.087123   0.844481\n",
       "3     1.115261    1.056059   0.821550    1.168965    1.081187   0.838211\n",
       "4     1.089298    1.043694   0.811556    1.158213    1.076203   0.832724\n",
       "5     1.065733    1.032343   0.802313    1.149096    1.071959   0.827845\n",
       "6     1.044144    1.021834   0.793693    1.141282    1.068308   0.823457\n",
       "7     1.024222    1.012039   0.785604    1.134531    1.065144   0.819477\n",
       "8     1.005728    1.002860   0.777978    1.128659    1.062384   0.815852\n",
       "9     0.988474    0.994220   0.770757    1.123526    1.059965   0.812530\n",
       "10    0.972306    0.986056   0.763896    1.119020    1.057837   0.809468\n",
       "11    0.957102    0.978316   0.757357    1.115052    1.055960   0.806633\n",
       "12    0.942756    0.970956   0.751111    1.111549    1.054300   0.803996\n",
       "13    0.929181    0.963940   0.745132    1.108451    1.052830   0.801533\n",
       "14    0.916302    0.957237   0.739396    1.105707    1.051526   0.799225\n",
       "15    0.904055    0.950818   0.733881    1.103275    1.050369   0.797059\n",
       "16    0.892383    0.944660   0.728572    1.101118    1.049342   0.795023\n",
       "17    0.881238    0.938743   0.723454    1.099208    1.048431   0.793105\n",
       "18    0.870576    0.933047   0.718511    1.097516    1.047624   0.791294\n",
       "19    0.860361    0.927557   0.713731    1.096020    1.046910   0.789579\n",
       "20    0.850558    0.922257   0.709104    1.094699    1.046279   0.787955\n",
       "21    0.841137    0.917135   0.704621    1.093538    1.045724   0.786414\n",
       "22    0.832071    0.912179   0.700273    1.092519    1.045237   0.784949\n",
       "23    0.823335    0.907378   0.696053    1.091631    1.044811   0.783554\n",
       "24    0.814908    0.902723   0.691951    1.090860    1.044442   0.782226\n",
       "25    0.806770    0.898204   0.687961    1.090196    1.044124   0.780959\n",
       "26    0.798903    0.893814   0.684075    1.089629    1.043853   0.779747\n",
       "27    0.791289    0.889544   0.680290    1.089152    1.043625   0.778588\n",
       "28    0.783915    0.885390   0.676599    1.088757    1.043435   0.777480\n",
       "29    0.776766    0.881343   0.672999    1.088436    1.043281   0.776418\n",
       "30    0.769828    0.877399   0.669483    1.088183    1.043160   0.775401\n",
       "31    0.763092    0.873551   0.666049    1.087994    1.043070   0.774426\n",
       "32    0.000000    0.000000   0.000000    0.000000    0.000000   0.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_svd.metrics_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rating only SVD RMSE on testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.114871427597778"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = rating_svd.predict(input_test)\n",
    "rmse = sqrt(mean_squared_error(input_test['rating'], np.array(pred)))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rating + NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_nlp_svd = my_SVD(lr=lr, reg=reg, n_epochs=n_epochs, n_factors=n_factors, early_stopping=early_stopping, shuffle=shuffle, min_rating=min_rating, max_rating=max_rating, min_delta=min_delta, mode=\"mixed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data...\n",
      "\n",
      "Preprocessing data...\n",
      "\n",
      "Epoch 1/100  | train_loss: 1.198 - train_rmse: 1.094 - train_mae: 0.854 - val_loss: 1.209 - val_rmse: 1.100 - val_mae: 0.858 - took 1.3 sec\n",
      "Epoch 2/100  | train_loss: 1.147 - train_rmse: 1.071 - train_mae: 0.837 - val_loss: 1.185 - val_rmse: 1.089 - val_mae: 0.848 - took 0.5 sec\n",
      "Epoch 3/100  | train_loss: 1.105 - train_rmse: 1.051 - train_mae: 0.822 - val_loss: 1.168 - val_rmse: 1.081 - val_mae: 0.840 - took 0.5 sec\n",
      "Epoch 4/100  | train_loss: 1.069 - train_rmse: 1.034 - train_mae: 0.809 - val_loss: 1.154 - val_rmse: 1.074 - val_mae: 0.834 - took 0.5 sec\n",
      "Epoch 5/100  | train_loss: 1.037 - train_rmse: 1.019 - train_mae: 0.797 - val_loss: 1.144 - val_rmse: 1.069 - val_mae: 0.828 - took 0.5 sec\n",
      "Epoch 6/100  | train_loss: 1.009 - train_rmse: 1.005 - train_mae: 0.786 - val_loss: 1.135 - val_rmse: 1.065 - val_mae: 0.823 - took 0.5 sec\n",
      "Epoch 7/100  | train_loss: 0.984 - train_rmse: 0.992 - train_mae: 0.776 - val_loss: 1.128 - val_rmse: 1.062 - val_mae: 0.819 - took 0.5 sec\n",
      "Epoch 8/100  | train_loss: 0.961 - train_rmse: 0.980 - train_mae: 0.766 - val_loss: 1.122 - val_rmse: 1.059 - val_mae: 0.815 - took 0.5 sec\n",
      "Epoch 9/100  | train_loss: 0.940 - train_rmse: 0.970 - train_mae: 0.758 - val_loss: 1.117 - val_rmse: 1.057 - val_mae: 0.811 - took 0.5 sec\n",
      "Epoch 10/100 | train_loss: 0.921 - train_rmse: 0.960 - train_mae: 0.749 - val_loss: 1.113 - val_rmse: 1.055 - val_mae: 0.808 - took 0.5 sec\n",
      "Epoch 11/100 | train_loss: 0.903 - train_rmse: 0.950 - train_mae: 0.741 - val_loss: 1.109 - val_rmse: 1.053 - val_mae: 0.805 - took 0.5 sec\n",
      "Epoch 12/100 | train_loss: 0.886 - train_rmse: 0.941 - train_mae: 0.734 - val_loss: 1.107 - val_rmse: 1.052 - val_mae: 0.803 - took 0.5 sec\n",
      "Epoch 13/100 | train_loss: 0.870 - train_rmse: 0.933 - train_mae: 0.727 - val_loss: 1.104 - val_rmse: 1.051 - val_mae: 0.800 - took 0.5 sec\n",
      "Epoch 14/100 | train_loss: 0.856 - train_rmse: 0.925 - train_mae: 0.720 - val_loss: 1.102 - val_rmse: 1.050 - val_mae: 0.798 - took 0.5 sec\n",
      "Epoch 15/100 | train_loss: 0.842 - train_rmse: 0.918 - train_mae: 0.714 - val_loss: 1.101 - val_rmse: 1.049 - val_mae: 0.795 - took 0.5 sec\n",
      "Epoch 16/100 | train_loss: 0.829 - train_rmse: 0.910 - train_mae: 0.707 - val_loss: 1.099 - val_rmse: 1.048 - val_mae: 0.793 - took 0.5 sec\n",
      "Epoch 17/100 | train_loss: 0.817 - train_rmse: 0.904 - train_mae: 0.701 - val_loss: 1.098 - val_rmse: 1.048 - val_mae: 0.791 - took 0.5 sec\n",
      "Epoch 18/100 | train_loss: 0.805 - train_rmse: 0.897 - train_mae: 0.696 - val_loss: 1.097 - val_rmse: 1.047 - val_mae: 0.790 - took 0.5 sec\n",
      "Epoch 19/100 | train_loss: 0.794 - train_rmse: 0.891 - train_mae: 0.690 - val_loss: 1.097 - val_rmse: 1.047 - val_mae: 0.788 - took 0.5 sec\n",
      "Epoch 20/100 | train_loss: 0.783 - train_rmse: 0.885 - train_mae: 0.685 - val_loss: 1.096 - val_rmse: 1.047 - val_mae: 0.786 - took 0.5 sec\n",
      "Epoch 21/100 | train_loss: 0.773 - train_rmse: 0.879 - train_mae: 0.680 - val_loss: 1.096 - val_rmse: 1.047 - val_mae: 0.785 - took 0.5 sec\n",
      "Epoch 22/100 | train_loss: 0.764 - train_rmse: 0.874 - train_mae: 0.675 - val_loss: 1.095 - val_rmse: 1.047 - val_mae: 0.783 - took 0.5 sec\n",
      "Epoch 23/100 | train_loss: 0.755 - train_rmse: 0.869 - train_mae: 0.670 - val_loss: 1.095 - val_rmse: 1.047 - val_mae: 0.782 - took 0.5 sec\n",
      "\n",
      "Training took 18 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<my_SVD.my_SVD at 0x1f9b3ff1d90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_nlp_svd.fit(input_train, input_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train Loss</th>\n",
       "      <th>Train RMSE</th>\n",
       "      <th>Train MAE</th>\n",
       "      <th>Valid Loss</th>\n",
       "      <th>Valid RMSE</th>\n",
       "      <th>Valid MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.197562</td>\n",
       "      <td>1.094332</td>\n",
       "      <td>0.854093</td>\n",
       "      <td>1.209405</td>\n",
       "      <td>1.099729</td>\n",
       "      <td>0.858467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.146736</td>\n",
       "      <td>1.070858</td>\n",
       "      <td>0.836778</td>\n",
       "      <td>1.185421</td>\n",
       "      <td>1.088770</td>\n",
       "      <td>0.848413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.104867</td>\n",
       "      <td>1.051127</td>\n",
       "      <td>0.821969</td>\n",
       "      <td>1.167870</td>\n",
       "      <td>1.080680</td>\n",
       "      <td>0.840458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.068960</td>\n",
       "      <td>1.033905</td>\n",
       "      <td>0.808817</td>\n",
       "      <td>1.154315</td>\n",
       "      <td>1.074390</td>\n",
       "      <td>0.833845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.037461</td>\n",
       "      <td>1.018558</td>\n",
       "      <td>0.796908</td>\n",
       "      <td>1.143549</td>\n",
       "      <td>1.069368</td>\n",
       "      <td>0.828181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.009401</td>\n",
       "      <td>1.004690</td>\n",
       "      <td>0.785986</td>\n",
       "      <td>1.134843</td>\n",
       "      <td>1.065290</td>\n",
       "      <td>0.823247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.984117</td>\n",
       "      <td>0.992027</td>\n",
       "      <td>0.775876</td>\n",
       "      <td>1.127714</td>\n",
       "      <td>1.061939</td>\n",
       "      <td>0.818874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.961128</td>\n",
       "      <td>0.980371</td>\n",
       "      <td>0.766454</td>\n",
       "      <td>1.121824</td>\n",
       "      <td>1.059162</td>\n",
       "      <td>0.814966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.940070</td>\n",
       "      <td>0.969572</td>\n",
       "      <td>0.757622</td>\n",
       "      <td>1.116927</td>\n",
       "      <td>1.056848</td>\n",
       "      <td>0.811430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.920664</td>\n",
       "      <td>0.959513</td>\n",
       "      <td>0.749303</td>\n",
       "      <td>1.112839</td>\n",
       "      <td>1.054912</td>\n",
       "      <td>0.808205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.902687</td>\n",
       "      <td>0.950099</td>\n",
       "      <td>0.741433</td>\n",
       "      <td>1.109420</td>\n",
       "      <td>1.053290</td>\n",
       "      <td>0.805241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.885958</td>\n",
       "      <td>0.941254</td>\n",
       "      <td>0.733961</td>\n",
       "      <td>1.106559</td>\n",
       "      <td>1.051931</td>\n",
       "      <td>0.802507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.870330</td>\n",
       "      <td>0.932915</td>\n",
       "      <td>0.726850</td>\n",
       "      <td>1.104168</td>\n",
       "      <td>1.050794</td>\n",
       "      <td>0.799971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.855679</td>\n",
       "      <td>0.925029</td>\n",
       "      <td>0.720063</td>\n",
       "      <td>1.102177</td>\n",
       "      <td>1.049846</td>\n",
       "      <td>0.797607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.841902</td>\n",
       "      <td>0.917552</td>\n",
       "      <td>0.713571</td>\n",
       "      <td>1.100529</td>\n",
       "      <td>1.049061</td>\n",
       "      <td>0.795398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.828910</td>\n",
       "      <td>0.910445</td>\n",
       "      <td>0.707352</td>\n",
       "      <td>1.099176</td>\n",
       "      <td>1.048416</td>\n",
       "      <td>0.793329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.816628</td>\n",
       "      <td>0.903675</td>\n",
       "      <td>0.701383</td>\n",
       "      <td>1.098080</td>\n",
       "      <td>1.047893</td>\n",
       "      <td>0.791388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.804991</td>\n",
       "      <td>0.897213</td>\n",
       "      <td>0.695643</td>\n",
       "      <td>1.097206</td>\n",
       "      <td>1.047476</td>\n",
       "      <td>0.789559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.793941</td>\n",
       "      <td>0.891034</td>\n",
       "      <td>0.690115</td>\n",
       "      <td>1.096528</td>\n",
       "      <td>1.047152</td>\n",
       "      <td>0.787834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.783429</td>\n",
       "      <td>0.885115</td>\n",
       "      <td>0.684784</td>\n",
       "      <td>1.096022</td>\n",
       "      <td>1.046911</td>\n",
       "      <td>0.786206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.773411</td>\n",
       "      <td>0.879438</td>\n",
       "      <td>0.679637</td>\n",
       "      <td>1.095667</td>\n",
       "      <td>1.046741</td>\n",
       "      <td>0.784667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.763848</td>\n",
       "      <td>0.873984</td>\n",
       "      <td>0.674660</td>\n",
       "      <td>1.095446</td>\n",
       "      <td>1.046636</td>\n",
       "      <td>0.783209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.754706</td>\n",
       "      <td>0.868738</td>\n",
       "      <td>0.669844</td>\n",
       "      <td>1.095345</td>\n",
       "      <td>1.046587</td>\n",
       "      <td>0.781827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Train Loss  Train RMSE  Train MAE  Valid Loss  Valid RMSE  Valid MAE\n",
       "0     1.197562    1.094332   0.854093    1.209405    1.099729   0.858467\n",
       "1     1.146736    1.070858   0.836778    1.185421    1.088770   0.848413\n",
       "2     1.104867    1.051127   0.821969    1.167870    1.080680   0.840458\n",
       "3     1.068960    1.033905   0.808817    1.154315    1.074390   0.833845\n",
       "4     1.037461    1.018558   0.796908    1.143549    1.069368   0.828181\n",
       "5     1.009401    1.004690   0.785986    1.134843    1.065290   0.823247\n",
       "6     0.984117    0.992027   0.775876    1.127714    1.061939   0.818874\n",
       "7     0.961128    0.980371   0.766454    1.121824    1.059162   0.814966\n",
       "8     0.940070    0.969572   0.757622    1.116927    1.056848   0.811430\n",
       "9     0.920664    0.959513   0.749303    1.112839    1.054912   0.808205\n",
       "10    0.902687    0.950099   0.741433    1.109420    1.053290   0.805241\n",
       "11    0.885958    0.941254   0.733961    1.106559    1.051931   0.802507\n",
       "12    0.870330    0.932915   0.726850    1.104168    1.050794   0.799971\n",
       "13    0.855679    0.925029   0.720063    1.102177    1.049846   0.797607\n",
       "14    0.841902    0.917552   0.713571    1.100529    1.049061   0.795398\n",
       "15    0.828910    0.910445   0.707352    1.099176    1.048416   0.793329\n",
       "16    0.816628    0.903675   0.701383    1.098080    1.047893   0.791388\n",
       "17    0.804991    0.897213   0.695643    1.097206    1.047476   0.789559\n",
       "18    0.793941    0.891034   0.690115    1.096528    1.047152   0.787834\n",
       "19    0.783429    0.885115   0.684784    1.096022    1.046911   0.786206\n",
       "20    0.773411    0.879438   0.679637    1.095667    1.046741   0.784667\n",
       "21    0.763848    0.873984   0.674660    1.095446    1.046636   0.783209\n",
       "22    0.754706    0.868738   0.669844    1.095345    1.046587   0.781827\n",
       "23    0.000000    0.000000   0.000000    0.000000    0.000000   0.000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_nlp_svd.metrics_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rating + NLP SVD RMSE on testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1209018073925991"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = rating_nlp_svd.predict(input_test)\n",
    "rmse = sqrt(mean_squared_error(input_test['rating'], pred))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = SVD(lr=lr, reg=reg, n_epochs=n_epochs, n_factors=n_factors, early_stopping=early_stopping, shuffle=shuffle, min_rating=min_rating, max_rating=max_rating, min_delta=min_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data...\n",
      "\n",
      "Preprocessing data...\n",
      "\n",
      "Epoch 1/100  | val_loss: 1.23 - val_rmse: 1.11 - val_mae: 0.86 - took 0.8 sec\n",
      "Epoch 2/100  | val_loss: 1.22 - val_rmse: 1.10 - val_mae: 0.86 - took 0.3 sec\n",
      "Epoch 3/100  | val_loss: 1.21 - val_rmse: 1.10 - val_mae: 0.85 - took 0.3 sec\n",
      "Epoch 4/100  | val_loss: 1.20 - val_rmse: 1.09 - val_mae: 0.85 - took 0.3 sec\n",
      "Epoch 5/100  | val_loss: 1.19 - val_rmse: 1.09 - val_mae: 0.84 - took 0.3 sec\n",
      "Epoch 6/100  | val_loss: 1.18 - val_rmse: 1.09 - val_mae: 0.84 - took 0.3 sec\n",
      "Epoch 7/100  | val_loss: 1.18 - val_rmse: 1.08 - val_mae: 0.83 - took 0.3 sec\n",
      "Epoch 8/100  | val_loss: 1.17 - val_rmse: 1.08 - val_mae: 0.83 - took 0.3 sec\n",
      "Epoch 9/100  | val_loss: 1.17 - val_rmse: 1.08 - val_mae: 0.83 - took 0.3 sec\n",
      "Epoch 10/100 | val_loss: 1.16 - val_rmse: 1.08 - val_mae: 0.82 - took 0.3 sec\n",
      "Epoch 11/100 | val_loss: 1.16 - val_rmse: 1.07 - val_mae: 0.82 - took 0.3 sec\n",
      "Epoch 12/100 | val_loss: 1.15 - val_rmse: 1.07 - val_mae: 0.82 - took 0.3 sec\n",
      "Epoch 13/100 | val_loss: 1.15 - val_rmse: 1.07 - val_mae: 0.81 - took 0.3 sec\n",
      "Epoch 14/100 | val_loss: 1.14 - val_rmse: 1.07 - val_mae: 0.81 - took 0.3 sec\n",
      "Epoch 15/100 | val_loss: 1.14 - val_rmse: 1.07 - val_mae: 0.81 - took 0.3 sec\n",
      "Epoch 16/100 | val_loss: 1.14 - val_rmse: 1.07 - val_mae: 0.81 - took 0.3 sec\n",
      "Epoch 17/100 | val_loss: 1.13 - val_rmse: 1.06 - val_mae: 0.81 - took 0.3 sec\n",
      "Epoch 18/100 | val_loss: 1.13 - val_rmse: 1.06 - val_mae: 0.80 - took 0.3 sec\n",
      "Epoch 19/100 | val_loss: 1.13 - val_rmse: 1.06 - val_mae: 0.80 - took 0.3 sec\n",
      "Epoch 20/100 | val_loss: 1.12 - val_rmse: 1.06 - val_mae: 0.80 - took 0.3 sec\n",
      "Epoch 21/100 | val_loss: 1.12 - val_rmse: 1.06 - val_mae: 0.80 - took 0.3 sec\n",
      "Epoch 22/100 | val_loss: 1.12 - val_rmse: 1.06 - val_mae: 0.80 - took 0.3 sec\n",
      "Epoch 23/100 | val_loss: 1.12 - val_rmse: 1.06 - val_mae: 0.79 - took 0.3 sec\n",
      "Epoch 24/100 | val_loss: 1.12 - val_rmse: 1.06 - val_mae: 0.79 - took 0.3 sec\n",
      "Epoch 25/100 | val_loss: 1.11 - val_rmse: 1.06 - val_mae: 0.79 - took 0.3 sec\n",
      "Epoch 26/100 | val_loss: 1.11 - val_rmse: 1.05 - val_mae: 0.79 - took 0.3 sec\n",
      "Epoch 27/100 | val_loss: 1.11 - val_rmse: 1.05 - val_mae: 0.79 - took 0.3 sec\n",
      "Epoch 28/100 | val_loss: 1.11 - val_rmse: 1.05 - val_mae: 0.79 - took 0.3 sec\n",
      "Epoch 29/100 | val_loss: 1.11 - val_rmse: 1.05 - val_mae: 0.79 - took 0.3 sec\n",
      "Epoch 30/100 | val_loss: 1.11 - val_rmse: 1.05 - val_mae: 0.78 - took 0.3 sec\n",
      "Epoch 31/100 | val_loss: 1.10 - val_rmse: 1.05 - val_mae: 0.78 - took 0.3 sec\n",
      "Epoch 32/100 | val_loss: 1.10 - val_rmse: 1.05 - val_mae: 0.78 - took 0.3 sec\n",
      "Epoch 33/100 | val_loss: 1.10 - val_rmse: 1.05 - val_mae: 0.78 - took 0.3 sec\n",
      "Epoch 34/100 | val_loss: 1.10 - val_rmse: 1.05 - val_mae: 0.78 - took 0.3 sec\n",
      "Epoch 35/100 | val_loss: 1.10 - val_rmse: 1.05 - val_mae: 0.78 - took 0.3 sec\n",
      "Epoch 36/100 | val_loss: 1.10 - val_rmse: 1.05 - val_mae: 0.78 - took 0.3 sec\n",
      "Epoch 37/100 | val_loss: 1.10 - val_rmse: 1.05 - val_mae: 0.78 - took 0.3 sec\n",
      "Epoch 38/100 | val_loss: 1.10 - val_rmse: 1.05 - val_mae: 0.78 - took 0.3 sec\n",
      "Epoch 39/100 | val_loss: 1.09 - val_rmse: 1.05 - val_mae: 0.78 - took 0.3 sec\n",
      "Epoch 40/100 | val_loss: 1.09 - val_rmse: 1.05 - val_mae: 0.77 - took 0.3 sec\n",
      "Epoch 41/100 | val_loss: 1.09 - val_rmse: 1.05 - val_mae: 0.77 - took 0.3 sec\n",
      "Epoch 42/100 | val_loss: 1.09 - val_rmse: 1.04 - val_mae: 0.77 - took 0.3 sec\n",
      "Epoch 43/100 | val_loss: 1.09 - val_rmse: 1.04 - val_mae: 0.77 - took 0.3 sec\n",
      "Epoch 44/100 | val_loss: 1.09 - val_rmse: 1.04 - val_mae: 0.77 - took 0.3 sec\n",
      "Epoch 45/100 | val_loss: 1.09 - val_rmse: 1.04 - val_mae: 0.77 - took 0.3 sec\n",
      "Epoch 46/100 | val_loss: 1.09 - val_rmse: 1.04 - val_mae: 0.77 - took 0.3 sec\n",
      "Epoch 47/100 | val_loss: 1.09 - val_rmse: 1.04 - val_mae: 0.77 - took 0.3 sec\n",
      "Epoch 48/100 | val_loss: 1.09 - val_rmse: 1.04 - val_mae: 0.77 - took 0.3 sec\n",
      "Epoch 49/100 | val_loss: 1.09 - val_rmse: 1.04 - val_mae: 0.77 - took 0.3 sec\n",
      "Epoch 50/100 | val_loss: 1.09 - val_rmse: 1.04 - val_mae: 0.77 - took 0.3 sec\n",
      "Epoch 51/100 | val_loss: 1.09 - val_rmse: 1.04 - val_mae: 0.77 - took 0.3 sec\n",
      "Epoch 52/100 | val_loss: 1.09 - val_rmse: 1.04 - val_mae: 0.77 - took 0.3 sec\n",
      "Epoch 53/100 | val_loss: 1.08 - val_rmse: 1.04 - val_mae: 0.77 - took 0.3 sec\n",
      "Epoch 54/100 | val_loss: 1.08 - val_rmse: 1.04 - val_mae: 0.77 - took 0.3 sec\n",
      "Epoch 55/100 | val_loss: 1.08 - val_rmse: 1.04 - val_mae: 0.76 - took 0.3 sec\n",
      "Epoch 56/100 | val_loss: 1.08 - val_rmse: 1.04 - val_mae: 0.76 - took 0.3 sec\n",
      "Epoch 57/100 | val_loss: 1.08 - val_rmse: 1.04 - val_mae: 0.76 - took 0.3 sec\n",
      "Epoch 58/100 | val_loss: 1.08 - val_rmse: 1.04 - val_mae: 0.76 - took 0.3 sec\n",
      "Epoch 59/100 | val_loss: 1.08 - val_rmse: 1.04 - val_mae: 0.76 - took 0.3 sec\n",
      "Epoch 60/100 | val_loss: 1.08 - val_rmse: 1.04 - val_mae: 0.76 - took 0.3 sec\n",
      "Epoch 61/100 | val_loss: 1.08 - val_rmse: 1.04 - val_mae: 0.76 - took 0.3 sec\n",
      "Epoch 62/100 | val_loss: 1.08 - val_rmse: 1.04 - val_mae: 0.76 - took 0.3 sec\n",
      "Epoch 63/100 | val_loss: 1.08 - val_rmse: 1.04 - val_mae: 0.76 - took 0.3 sec\n",
      "Epoch 64/100 | val_loss: 1.08 - val_rmse: 1.04 - val_mae: 0.76 - took 0.3 sec\n",
      "Epoch 65/100 | val_loss: 1.08 - val_rmse: 1.04 - val_mae: 0.76 - took 0.3 sec\n",
      "Epoch 66/100 | val_loss: 1.08 - val_rmse: 1.04 - val_mae: 0.76 - took 0.3 sec\n",
      "Epoch 67/100 | val_loss: 1.08 - val_rmse: 1.04 - val_mae: 0.76 - took 0.3 sec\n",
      "Epoch 68/100 | val_loss: 1.08 - val_rmse: 1.04 - val_mae: 0.76 - took 0.3 sec\n",
      "Epoch 69/100 | val_loss: 1.08 - val_rmse: 1.04 - val_mae: 0.76 - took 0.3 sec\n",
      "\n",
      "Training took 23 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<funk_svd.svd.SVD at 0x1f9cc7f9e80>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd.fit(X=input_train, X_val=input_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loss</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.231632</td>\n",
       "      <td>1.109789</td>\n",
       "      <td>0.864312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.218699</td>\n",
       "      <td>1.103947</td>\n",
       "      <td>0.857533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.208083</td>\n",
       "      <td>1.099128</td>\n",
       "      <td>0.851686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.198936</td>\n",
       "      <td>1.094959</td>\n",
       "      <td>0.846500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.190851</td>\n",
       "      <td>1.091261</td>\n",
       "      <td>0.841833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1.080105</td>\n",
       "      <td>1.039281</td>\n",
       "      <td>0.759550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>1.079878</td>\n",
       "      <td>1.039172</td>\n",
       "      <td>0.759170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>1.079665</td>\n",
       "      <td>1.039070</td>\n",
       "      <td>0.758800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1.079466</td>\n",
       "      <td>1.038974</td>\n",
       "      <td>0.758440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Loss      RMSE       MAE\n",
       "0   1.231632  1.109789  0.864312\n",
       "1   1.218699  1.103947  0.857533\n",
       "2   1.208083  1.099128  0.851686\n",
       "3   1.198936  1.094959  0.846500\n",
       "4   1.190851  1.091261  0.841833\n",
       "..       ...       ...       ...\n",
       "65  1.080105  1.039281  0.759550\n",
       "66  1.079878  1.039172  0.759170\n",
       "67  1.079665  1.039070  0.758800\n",
       "68  1.079466  1.038974  0.758440\n",
       "69  0.000000  0.000000  0.000000\n",
       "\n",
       "[70 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd.metrics_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base SVD RMSE on testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1116723740980092"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = svd.predict(input_test)\n",
    "rmse = sqrt(mean_squared_error(input_test['rating'], pred))\n",
    "rmse"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fcf1d46d271c46101d6967829d4a5f475342a2ce08e4944f989fbcdc9bb23690"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
