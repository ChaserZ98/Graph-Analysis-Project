{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import prettytable as pt\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "| database() |\n",
      "+------------+\n",
      "|   cs550    |\n",
      "+------------+\n"
     ]
    }
   ],
   "source": [
    "\n",
    "db = pymysql.connect(\n",
    "    host = 'localhost',\n",
    "    user = 'root',\n",
    "    password = 'mysql'\n",
    ")\n",
    "cursor = db.cursor()\n",
    "cursor.execute(\"create database if not exists cs550\")\n",
    "\n",
    "\n",
    "cursor.execute(\"use cs550\")\n",
    "\n",
    "cursor.execute(\"select database()\")\n",
    "\n",
    "table = pt.from_db_cursor(cursor)\n",
    "print(table)\n",
    "\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir = \"../Data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some data about two large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalReviews = 157260921\n",
    "totalReviewPath = os.path.join(dataDir, \"All_Amazon_Review_5.json\")\n",
    "\n",
    "totalMeta = 15023060\n",
    "totalMetaPath = os.path.join(dataDir, \"All_Amazon_Meta.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Analysis on Review Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|█████████▉| 157M/157M [30:48<00:00, 85.1kit/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'overall': 'float', 'verified': 'bool', 'reviewTime': ('str', 1, 11), 'reviewerID': ('str', 11586, 20), 'asin': ('str', 0, 10), 'reviewerName': ('str', 28782337, 1725), 'reviewText': ('str', 140467823, 35094), 'summary': ('str', 151619415, 1730), 'unixReviewTime': 'int', 'vote': ('str', 257028, 6), 'image': ('list', 136488683, 508), 'style': ('dict', 2377009, 7)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# simple analysis on review dataset\n",
    "totalReviews = 157260921\n",
    "columns = ['verified', 'image', 'style', 'asin', 'reviewerID', 'overall', 'reviewText', 'reviewTime', 'unixReviewTime', 'summary', 'reviewerName', 'vote']\n",
    "column_dict = dict()\n",
    "style = set()\n",
    "dataPath = totalReviewPath\n",
    "with open(dataPath, 'r', encoding=\"utf-8\") as f:\n",
    "    with tqdm(total=totalReviews, desc=\"Processing\", leave=True, unit_scale=True) as pbar:\n",
    "        index = 0\n",
    "        line = f.readline()\n",
    "        while line:\n",
    "            pbar.update(1)\n",
    "            row = json.loads(line)\n",
    "            for key in row.keys():\n",
    "                valueType = type(row[key])\n",
    "                if key not in column_dict:\n",
    "                    if valueType in [list, dict, str]:\n",
    "                        column_dict[key] = (valueType.__name__, index, len(row[key]))\n",
    "                    else:\n",
    "                        column_dict[key] = (valueType.__name__)\n",
    "                else:\n",
    "                    if valueType in [list, dict, str] and len(row[key]) > column_dict[key][2]:\n",
    "                        column_dict[key] = (valueType.__name__, index, len(row[key]))\n",
    "            line = f.readline()\n",
    "            index += 1\n",
    "print(column_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|█████████▉| 157M/157M [23:15<00:00, 113kit/s]  \n"
     ]
    }
   ],
   "source": [
    "def splitColumns(selectedKeys:list, dataPath:str, totalRowNum:int, outputPath:str):\n",
    "    \"\"\"function used to split dataset\n",
    "\n",
    "    Args:\n",
    "        selectedKeys (list): list contains column names\n",
    "        dataPath (str): input file path\n",
    "        totalRowNum (int): number of rows\n",
    "        outputPath (str): output file path\n",
    "    \"\"\"\n",
    "    with open(outputPath, \"w\", newline=\"\") as outputFile:\n",
    "        writer = csv.DictWriter(outputFile, selectedKeys)\n",
    "        writer.writeheader()\n",
    "        with open(dataPath, encoding=\"utf-8\") as file:\n",
    "            with tqdm(total=totalRowNum, desc=\"Processing\", leave=True, unit_scale=True) as pbar:\n",
    "                for line in file:\n",
    "                    row = json.loads(line)\n",
    "                    writer.writerow(dict([(key, row.get(key)) for key in selectedKeys]))\n",
    "                    pbar.update()\n",
    "\n",
    "totalReviews = 157260921 # total number of reviews\n",
    "dataPath = \"../Data/All_Amazon_Review_5.json\" # input dataset path\n",
    "outputPath = \"../Data/All_Amazon_Review_User_Item_Rating.csv\" # output datset path\n",
    "selectedKeys = ['reviewerID', 'asin', 'overall'] # choose columns to split from dataset\n",
    "splitColumns(selectedKeys, dataPath, totalReviews, outputPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset File: ../Data\\All_Amazon_Review_5.json\n",
      "Total Size: 157260921\n",
      "Selected Keys: ['reviewerID', 'asin', 'overall']\n",
      "Output File: ../Data\\All_Amazon_Review_User_Item_Rating.csv\n",
      "Number of processes: 12\n",
      "Size of each chunk: 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|█████████▉| 157M/157M [11:32<00:00, 227kit/s]      \n"
     ]
    }
   ],
   "source": [
    "import multiprocess_methods\n",
    "from multiprocessing import cpu_count\n",
    "if __name__ == '__main__':\n",
    "    dataPath = os.path.join(dataDir, \"All_Amazon_Review_5.json\")\n",
    "    totalSize = 157260921\n",
    "    selectedKeys = ['reviewerID', 'asin', 'overall']\n",
    "    outputPath = os.path.join(dataDir, \"All_Amazon_Review_User_Item_Rating.csv\")\n",
    "    processNum = cpu_count() # number of processes (customize this variable based on different CPUs)\n",
    "    chunkSize = 1024 # the size of each chunk splitted from the iterable\n",
    "    printParameters = True\n",
    "    multiprocess_methods.splitDataset(dataPath, totalSize, selectedKeys, outputPath, processNum, chunkSize, printParameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 2.50M/2.50M [01:39<00:00, 25.0kit/s]\n"
     ]
    }
   ],
   "source": [
    "def sampleData(sampleSize:int, totalSize:int, dataPath:str, outputPath:str):\n",
    "    \"\"\"function used to sample a small dataset from a large csv file\n",
    "\n",
    "    Args:\n",
    "        sampleSize (int): the size of the sample\n",
    "        totalSize (int): the size of the original dataset\n",
    "        dataPath (str): original dataset path\n",
    "        outputPath (str): sample dataset output path\n",
    "    \"\"\"\n",
    "    sampleIndicesList = sorted(random.sample(range(totalSize), sampleSize))\n",
    "    sampleIndex = 0\n",
    "    with open(dataPath, encoding=\"utf-8\", newline='') as file:\n",
    "        csvReader = csv.reader(file)\n",
    "        csvHeader = next(csvReader) # read header\n",
    "        with open(outputPath, \"w\", newline='') as outputFile:\n",
    "            csvWriter = csv.writer(outputFile, csvHeader)\n",
    "            csvWriter.writerow(csvHeader) # write header\n",
    "            with tqdm(total=sampleSize, desc=\"Processing\", leave=True, unit_scale=True) as pbar:\n",
    "                for index, row in enumerate(csvReader):\n",
    "                    if index == sampleIndicesList[sampleIndex]:\n",
    "                        csvWriter.writerow(row)\n",
    "                        sampleIndex += 1\n",
    "                        pbar.update(1)\n",
    "                    if sampleIndex == sampleSize:\n",
    "                        break\n",
    "                    \n",
    "\n",
    "data_dir = \"../Data/\"\n",
    "input_csv_path = os.path.join(data_dir, \"All_Amazon_Review_User_Item_Rating.csv\")\n",
    "output_csv_path = os.path.join(data_dir, \"sampled_data.csv\")\n",
    "total_records = 157260921\n",
    "required_records = 2500000\n",
    "sampleData(required_records, total_records, input_csv_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mg:\\Python Projects\\Recommender-System-Project\\Code\\preprocess.ipynb Cell 18'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/Python%20Projects/Recommender-System-Project/Code/preprocess.ipynb#ch0000019?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlinecache\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/g%3A/Python%20Projects/Recommender-System-Project/Code/preprocess.ipynb#ch0000019?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(linecache\u001b[39m.\u001b[39;49mgetline(\u001b[39m\"\u001b[39;49m\u001b[39m../Data/All_Amazon_Review_5.json\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m20\u001b[39;49m))\n",
      "File \u001b[1;32mD:\\Python\\Python39\\lib\\linecache.py:30\u001b[0m, in \u001b[0;36mgetline\u001b[1;34m(filename, lineno, module_globals)\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/Python/Python39/lib/linecache.py?line=25'>26</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgetline\u001b[39m(filename, lineno, module_globals\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     <a href='file:///d%3A/Python/Python39/lib/linecache.py?line=26'>27</a>\u001b[0m     \u001b[39m\"\"\"Get a line for a Python source file from the cache.\u001b[39;00m\n\u001b[0;32m     <a href='file:///d%3A/Python/Python39/lib/linecache.py?line=27'>28</a>\u001b[0m \u001b[39m    Update the cache if it doesn't contain an entry for this file already.\"\"\"\u001b[39;00m\n\u001b[1;32m---> <a href='file:///d%3A/Python/Python39/lib/linecache.py?line=29'>30</a>\u001b[0m     lines \u001b[39m=\u001b[39m getlines(filename, module_globals)\n\u001b[0;32m     <a href='file:///d%3A/Python/Python39/lib/linecache.py?line=30'>31</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m1\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m lineno \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(lines):\n\u001b[0;32m     <a href='file:///d%3A/Python/Python39/lib/linecache.py?line=31'>32</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m lines[lineno \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32mD:\\Python\\Python39\\lib\\linecache.py:46\u001b[0m, in \u001b[0;36mgetlines\u001b[1;34m(filename, module_globals)\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/Python/Python39/lib/linecache.py?line=42'>43</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m cache[filename][\u001b[39m2\u001b[39m]\n\u001b[0;32m     <a href='file:///d%3A/Python/Python39/lib/linecache.py?line=44'>45</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///d%3A/Python/Python39/lib/linecache.py?line=45'>46</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m updatecache(filename, module_globals)\n\u001b[0;32m     <a href='file:///d%3A/Python/Python39/lib/linecache.py?line=46'>47</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mMemoryError\u001b[39;00m:\n\u001b[0;32m     <a href='file:///d%3A/Python/Python39/lib/linecache.py?line=47'>48</a>\u001b[0m     clearcache()\n",
      "File \u001b[1;32mD:\\Python\\Python39\\lib\\linecache.py:137\u001b[0m, in \u001b[0;36mupdatecache\u001b[1;34m(filename, module_globals)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Python/Python39/lib/linecache.py?line=134'>135</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    <a href='file:///d%3A/Python/Python39/lib/linecache.py?line=135'>136</a>\u001b[0m     \u001b[39mwith\u001b[39;00m tokenize\u001b[39m.\u001b[39mopen(fullname) \u001b[39mas\u001b[39;00m fp:\n\u001b[1;32m--> <a href='file:///d%3A/Python/Python39/lib/linecache.py?line=136'>137</a>\u001b[0m         lines \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39;49mreadlines()\n\u001b[0;32m    <a href='file:///d%3A/Python/Python39/lib/linecache.py?line=137'>138</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[0;32m    <a href='file:///d%3A/Python/Python39/lib/linecache.py?line=138'>139</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m []\n",
      "File \u001b[1;32mD:\\Python\\Python39\\lib\\codecs.py:319\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Python/Python39/lib/codecs.py?line=313'>314</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_buffer_decode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, errors, final):\n\u001b[0;32m    <a href='file:///d%3A/Python/Python39/lib/codecs.py?line=314'>315</a>\u001b[0m     \u001b[39m# Overwrite this method in subclasses: It must decode input\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/Python/Python39/lib/codecs.py?line=315'>316</a>\u001b[0m     \u001b[39m# and return an (output, length consumed) tuple\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/Python/Python39/lib/codecs.py?line=316'>317</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m\n\u001b[1;32m--> <a href='file:///d%3A/Python/Python39/lib/codecs.py?line=318'>319</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    <a href='file:///d%3A/Python/Python39/lib/codecs.py?line=319'>320</a>\u001b[0m     \u001b[39m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/Python/Python39/lib/codecs.py?line=320'>321</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer \u001b[39m+\u001b[39m \u001b[39minput\u001b[39m\n\u001b[0;32m    <a href='file:///d%3A/Python/Python39/lib/codecs.py?line=321'>322</a>\u001b[0m     (result, consumed) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer_decode(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merrors, final)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fcf1d46d271c46101d6967829d4a5f475342a2ce08e4944f989fbcdc9bb23690"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
