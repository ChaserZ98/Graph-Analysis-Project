{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import prettytable as pt\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "| database() |\n",
      "+------------+\n",
      "|   cs550    |\n",
      "+------------+\n"
     ]
    }
   ],
   "source": [
    "\n",
    "db = pymysql.connect(\n",
    "    host = 'localhost',\n",
    "    user = 'root',\n",
    "    password = 'mysql'\n",
    ")\n",
    "cursor = db.cursor()\n",
    "cursor.execute(\"create database if not exists cs550\")\n",
    "\n",
    "\n",
    "cursor.execute(\"use cs550\")\n",
    "\n",
    "cursor.execute(\"select database()\")\n",
    "\n",
    "table = pt.from_db_cursor(cursor)\n",
    "print(table)\n",
    "\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir = \"../Data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some data about two large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalReviews = 157260921\n",
    "totalReviewPath = os.path.join(dataDir, \"All_Amazon_Review_5.json\")\n",
    "\n",
    "totalMeta = 15023060\n",
    "totalMetaPath = os.path.join(dataDir, \"All_Amazon_Meta.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Analysis on Review Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|█████████▉| 157M/157M [30:48<00:00, 85.1kit/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'overall': 'float', 'verified': 'bool', 'reviewTime': ('str', 1, 11), 'reviewerID': ('str', 11586, 20), 'asin': ('str', 0, 10), 'reviewerName': ('str', 28782337, 1725), 'reviewText': ('str', 140467823, 35094), 'summary': ('str', 151619415, 1730), 'unixReviewTime': 'int', 'vote': ('str', 257028, 6), 'image': ('list', 136488683, 508), 'style': ('dict', 2377009, 7)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# simple analysis on review dataset\n",
    "totalReviews = 157260921\n",
    "columns = ['verified', 'image', 'style', 'asin', 'reviewerID', 'overall', 'reviewText', 'reviewTime', 'unixReviewTime', 'summary', 'reviewerName', 'vote']\n",
    "column_dict = dict()\n",
    "style = set()\n",
    "dataPath = totalReviewPath\n",
    "with open(dataPath, 'r', encoding=\"utf-8\") as f:\n",
    "    with tqdm(total=totalReviews, desc=\"Processing\", leave=True, unit_scale=True) as pbar:\n",
    "        index = 0\n",
    "        line = f.readline()\n",
    "        while line:\n",
    "            pbar.update(1)\n",
    "            row = json.loads(line)\n",
    "            for key in row.keys():\n",
    "                valueType = type(row[key])\n",
    "                if key not in column_dict:\n",
    "                    if valueType in [list, dict, str]:\n",
    "                        column_dict[key] = (valueType.__name__, index, len(row[key]))\n",
    "                    else:\n",
    "                        column_dict[key] = (valueType.__name__)\n",
    "                else:\n",
    "                    if valueType in [list, dict, str] and len(row[key]) > column_dict[key][2]:\n",
    "                        column_dict[key] = (valueType.__name__, index, len(row[key]))\n",
    "            line = f.readline()\n",
    "            index += 1\n",
    "print(column_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|█████████▉| 157M/157M [23:15<00:00, 113kit/s]  \n"
     ]
    }
   ],
   "source": [
    "def splitColumns(selectedKeys:list, dataPath:str, totalRowNum:int, outputPath:str):\n",
    "    \"\"\"function used to split dataset\n",
    "\n",
    "    Args:\n",
    "        selectedKeys (list): list contains column names\n",
    "        dataPath (str): input file path\n",
    "        totalRowNum (int): number of rows\n",
    "        outputPath (str): output file path\n",
    "    \"\"\"\n",
    "    with open(outputPath, \"w\", newline=\"\") as outputFile:\n",
    "        writer = csv.DictWriter(outputFile, selectedKeys)\n",
    "        writer.writeheader()\n",
    "        with open(dataPath, encoding=\"utf-8\") as file:\n",
    "            with tqdm(total=totalRowNum, desc=\"Processing\", leave=True, unit_scale=True) as pbar:\n",
    "                for line in file:\n",
    "                    row = json.loads(line)\n",
    "                    writer.writerow(dict([(key, row.get(key)) for key in selectedKeys]))\n",
    "                    pbar.update()\n",
    "\n",
    "totalReviews = 157260921 # total number of reviews\n",
    "dataPath = \"../Data/All_Amazon_Review_5.json\" # input dataset path\n",
    "outputPath = \"../Data/All_Amazon_Review_User_Item_Rating.csv\" # output datset path\n",
    "selectedKeys = ['reviewerID', 'asin', 'overall'] # choose columns to split from dataset\n",
    "splitColumns(selectedKeys, dataPath, totalReviews, outputPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset File: ../Data\\All_Amazon_Review_5.json\n",
      "Total Size: 157260921\n",
      "Selected Keys: ['reviewerID', 'asin', 'overall']\n",
      "Output File: ../Data\\All_Amazon_Review_User_Item_Rating.csv\n",
      "Number of processes: 12\n",
      "Size of each chunk: 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|█████████▉| 157M/157M [19:50<00:00, 132kit/s]    \n"
     ]
    }
   ],
   "source": [
    "import multiprocess_methods\n",
    "from multiprocessing import cpu_count\n",
    "if __name__ == '__main__':\n",
    "    dataPath = os.path.join(dataDir, \"All_Amazon_Review_5.json\")\n",
    "    totalSize = 157260921\n",
    "    selectedKeys = ['reviewerID', 'asin', 'overall']\n",
    "    outputPath = os.path.join(dataDir, \"All_Amazon_Review_User_Item_Rating.csv\")\n",
    "    processNum = cpu_count() # number of processes (customize this variable based on different CPUs)\n",
    "    chunkSize = 1024 # the size of each chunk splitted from the iterable\n",
    "    printParameters = True\n",
    "    multiprocess_methods.splitDataset(dataPath, totalSize, selectedKeys, outputPath, processNum, chunkSize, printParameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 2.50M/2.50M [01:53<00:00, 22.0kit/s]\n"
     ]
    }
   ],
   "source": [
    "def sampleData(sampleSize:int, totalSize:int, dataPath:str, outputPath:str):\n",
    "    \"\"\"function used to sample a small dataset from a large csv file\n",
    "\n",
    "    Args:\n",
    "        sampleSize (int): the size of the sample\n",
    "        totalSize (int): the size of the original dataset\n",
    "        dataPath (str): original dataset path\n",
    "        outputPath (str): sample dataset output path\n",
    "    \"\"\"\n",
    "    sampleIndicesList = sorted(random.sample(range(totalSize), sampleSize))\n",
    "    sampleIndex = 0\n",
    "    with open(dataPath, encoding=\"utf-8\", newline='') as file:\n",
    "        csvReader = csv.reader(file)\n",
    "        csvHeader = next(csvReader) # read header\n",
    "        with open(outputPath, \"w\", newline='') as outputFile:\n",
    "            csvWriter = csv.writer(outputFile, csvHeader)\n",
    "            csvWriter.writerow(csvHeader) # write header\n",
    "            with tqdm(total=sampleSize, desc=\"Processing\", leave=True, unit_scale=True) as pbar:\n",
    "                for index, row in enumerate(csvReader):\n",
    "                    if index == sampleIndicesList[sampleIndex]:\n",
    "                        csvWriter.writerow(row)\n",
    "                        sampleIndex += 1\n",
    "                        pbar.update()\n",
    "                    if sampleIndex == sampleSize:\n",
    "                        break\n",
    "                    \n",
    "\n",
    "data_dir = \"../Data/\"\n",
    "input_csv_path = os.path.join(data_dir, \"All_Amazon_Review_User_Item_Rating.csv\")\n",
    "output_csv_path = os.path.join(data_dir, \"sampled_data.csv\")\n",
    "total_records = 157260921\n",
    "required_records = 2500000\n",
    "sampleData(required_records, total_records, input_csv_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset File: ../Data/All_Amazon_Review_User_Item_Rating.csv\n",
      "Total Size: 157260921\n",
      "Output File: ../Data/sampled_data.csv\n",
      "Sample size: 2500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 2.50M/2.50M [01:59<00:00, 20.8kit/s]\n"
     ]
    }
   ],
   "source": [
    "import multiprocess_methods\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data_dir = \"../Data/\"\n",
    "    input_csv_path = os.path.join(data_dir, \"All_Amazon_Review_User_Item_Rating.csv\")\n",
    "    output_csv_path = os.path.join(data_dir, \"sampled_data.csv\")\n",
    "    total_records = 157260921\n",
    "    required_records = 2500000\n",
    "    printParameters = True\n",
    "    multiprocess_methods.sampleDataset(input_csv_path, total_records, output_csv_path, required_records, printParameters)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fcf1d46d271c46101d6967829d4a5f475342a2ce08e4944f989fbcdc9bb23690"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
